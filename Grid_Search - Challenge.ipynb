{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of sales\n",
    "\n",
    "### Problem Statement\n",
    "This dataset represents sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store are available. The aim is to build a predictive model and find out the sales of each product at a particular store.\n",
    "\n",
    "|Variable|Description|\n",
    "|: ------------- |:-------------|\n",
    "|Item_Identifier|Unique product ID|\n",
    "|Item_Weight|Weight of product|\n",
    "|Item_Fat_Content|Whether the product is low fat or not|\n",
    "|Item_Visibility|The % of total display area of all products in a store allocated to the particular product|\n",
    "|Item_Type|The category to which the product belongs|\n",
    "|Item_MRP|Maximum Retail Price (list price) of the product|\n",
    "|Outlet_Identifier|Unique store ID|\n",
    "|Outlet_Establishment_Year|The year in which store was established|\n",
    "|Outlet_Size|The size of the store in terms of ground area covered|\n",
    "|Outlet_Location_Type|The type of city in which the store is located|\n",
    "|Outlet_Type|Whether the outlet is just a grocery store or some sort of supermarket|\n",
    "|Item_Outlet_Sales|Sales of the product in the particulat store. This is the outcome variable to be predicted.|\n",
    "\n",
    "Please note that the data may have missing values as some stores might not report all the data due to technical glitches. Hence, it will be required to treat them accordingly.\n",
    "\n",
    "\n",
    "\n",
    "### Explore the problem in following stages:\n",
    "\n",
    "1. Hypothesis Generation – understanding the problem better by brainstorming possible factors that can impact the outcome\n",
    "2. Data Exploration – looking at categorical and continuous feature summaries and making inferences about the data.\n",
    "3. Data Cleaning – imputing missing values in the data and checking for outliers\n",
    "4. Feature Engineering – modifying existing variables and creating new ones for analysis\n",
    "5. Model Building – making predictive models on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have covered data preparation and feature engineering two weeks ago. Now, it's time to do some predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "## Task\n",
    "Make a baseline model. Baseline models help us set a benchmark to gauge the performance of our future models. If your new model is below the baseline, something has gone wrong, and you should check your data.\n",
    "\n",
    "To make a baseline model, run a simple regression model without altering the default parameters in sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8523, 1607) (8523,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dataframe = pd.read_csv('df_final.csv', dtype={'Item_Weight':'float64'}, low_memory=False)\n",
    "data = dataframe.values\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "y = y.astype(int)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "model = LogisticRegression()\n",
    "result = model.fit(X_scaled,y)\n",
    "model.score(X_scaled,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Split your data in 80% train set and 20% test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Use grid_search to find the best value of the parameter `alpha` for Ridge and Lasso regressions from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import RepeatedKFold, RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=2, random_state=1)\n",
    "\n",
    "space = {\n",
    "    'alpha': loguniform(1e-3, 100)\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV with reduced n_iter and n_jobs for efficiency\n",
    "ridge_model = Ridge()\n",
    "ridge_search = RandomizedSearchCV(ridge_model, space, n_iter=10, scoring='neg_mean_squared_error', n_jobs=-1, cv=cv, random_state=1)\n",
    "ridge_result = ridge_search.fit(X_train, y_train)\n",
    "# Fit model\n",
    "lasso_model = Lasso(max_iter=5000)\n",
    "lasso_search = RandomizedSearchCV(lasso_model, space, n_iter=10, scoring='neg_mean_squared_error', n_jobs=-1, cv=cv, random_state=1)\n",
    "lasso_result = lasso_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a range of discrete alpha values for grid search\n",
    "space = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "ridge_search = GridSearchCV(ridge_model, space, scoring='neg_mean_squared_error', n_jobs=-1, cv=cv)\n",
    "ridge_result = ridge_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for Ridge: {'alpha': 0.01}\n",
      "Best Score for Ridge: -0.00023726807804257008\n",
      "Best Parameters for Lasso: {'alpha': 0.0010013176560941261}\n",
      "Best Score for Lasso: -3.805989732935029e-06\n"
     ]
    }
   ],
   "source": [
    "# Best parameters found\n",
    "print(\"Best Parameters for Ridge:\", ridge_result.best_params_)\n",
    "print(\"Best Score for Ridge:\", ridge_result.best_score_)\n",
    "\n",
    "print(\"Best Parameters for Lasso:\", lasso_result.best_params_)\n",
    "print(\"Best Score for Lasso:\", lasso_result.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Using the model from grid_search, predict the values in the test set and compare against your benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Mean Squared Error: 1.915904310693906e-05\n",
      "Lasso Mean Squared Error: 3.6182718411989197e-06\n",
      "Benchmark Mean Squared Error: 0.14772664182509457\n",
      "Improvement over benchmark (Ridge): 0.14770748278198764\n",
      "Improvement over benchmark (Lasso): 0.14772302355325337\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming 'ridge_result' and 'lasso_result' are your tuned models\n",
    "\n",
    "# Select the best Ridge model from RandomizedSearchCV\n",
    "best_ridge = ridge_result.best_estimator_\n",
    "\n",
    "# Predict on the test set\n",
    "ridge_predictions = best_ridge.predict(X_test)\n",
    "\n",
    "# Calculate the error for Ridge regression\n",
    "ridge_mse = mean_squared_error(y_test, ridge_predictions)\n",
    "print(f\"Ridge Mean Squared Error: {ridge_mse}\")\n",
    "\n",
    "# Repeat for Lasso\n",
    "best_lasso = lasso_result.best_estimator_\n",
    "lasso_predictions = best_lasso.predict(X_test)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_predictions)\n",
    "print(f\"Lasso Mean Squared Error: {lasso_mse}\")\n",
    "\n",
    "# Compare to your benchmark MSE\n",
    "benchmark_mse = mean_squared_error(y_test, [y_train.mean()] * len(y_test))  # Example: benchmark as mean of y_train\n",
    "print(f\"Benchmark Mean Squared Error: {benchmark_mse}\")\n",
    "\n",
    "print(f\"Improvement over benchmark (Ridge): {benchmark_mse - ridge_mse}\")\n",
    "print(f\"Improvement over benchmark (Lasso): {benchmark_mse - lasso_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
